"""
Real-world integration test: Compare PyRegression to R's lm() on actual data.

This is a proper statistical validation test using the classic mtcars dataset,
comparing our results to R's gold-standard output for regulatory compliance.
"""

import pytest
import numpy as np
import pandas as pd
from pyregression._backends import get_backend, list_available_backends


# ============================================================================
# R REFERENCE OUTPUT (generated with R 4.4.2)
# ============================================================================
# R code:
# data(mtcars)
# model <- lm(mpg ~ wt + hp + cyl, data=mtcars)
# 
# These values were generated by running generate_r_reference.R

R_REFERENCE = {
    'coefficients': np.array([
        38.7517873729,  # Intercept
        -3.1669731107,  # wt
        -0.0180381021,  # hp
        -0.9416168120   # cyl
    ]),
    'std_errors': np.array([
        1.78686,
        0.74058,
        0.01188,
        0.55092
    ]),
    'residual_se': 2.512,
    'r_squared': 0.8431,
    'adj_r_squared': 0.8263,
    'df_residual': 28,
    'rank': 4,
    # First 10 residuals
    'residuals_first10': np.array([
        -1.8204257,
        -1.0128476,
        -3.1603990,
        0.4639233,
        1.5322025,
        -2.1503588,
        -1.1934238,
        0.6356864,
        -0.4957351,
        -0.7890124
    ]),
    # First 10 fitted values  
    'fitted_first10': np.array([
        22.8204257,
        22.0128476,
        25.9603990,
        20.9360767,
        17.1677975,
        20.2503588,
        15.4934238,
        23.7643136,
        23.2957351,
        19.9890124
    ])
}


# ============================================================================
# MTCARS DATASET
# ============================================================================
# Classic automotive dataset (Henderson and Velleman, 1981)
# 32 automobiles (1973–74 models)
MTCARS = pd.DataFrame({
    'mpg': [21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,
            16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5,
            15.2, 13.3, 19.2, 27.3, 26.0, 30.4, 15.8, 19.7, 15.0, 21.4],
    'cyl': [6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,
            8, 8, 8, 4, 4, 4, 8, 6, 8, 4],
    'hp': [110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180,
           205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113,
           264, 175, 335, 109],
    'wt': [2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150,
           3.440, 3.440, 4.070, 3.730, 3.780, 5.250, 5.424, 5.345, 2.200,
           1.615, 1.835, 2.465, 3.520, 3.435, 3.840, 3.845, 1.935, 2.140,
           1.513, 3.170, 2.770, 3.570, 2.780]
})


class TestRealWorldRegression:
    """Integration tests using real statistical datasets."""
    
    def test_mtcars_cpu_vs_r(self):
        """
        Test CPU backend produces R-equivalent results on mtcars data.
        
        This is the gold standard test - if this passes, we can submit
        to regulatory agencies with confidence.
        """
        # Prepare data
        y = MTCARS['mpg'].values
        X = MTCARS[['wt', 'hp', 'cyl']].values
        
        # Fit model
        backend = get_backend('cpu')
        result = backend.fit_linear_model(X, y)
        
        # ===== CRITICAL VALIDATION =====
        # These must match R to machine precision for regulatory compliance
        
        # 1. Coefficients (most important!)
        print("\n" + "="*70)
        print("COEFFICIENT COMPARISON (CPU vs R)")
        print("="*70)
        print(f"{'Variable':<12} {'PyRegression':<15} {'R lm()':<15} {'Diff':<12}")
        print("-"*70)
        vars = ['Intercept', 'wt', 'hp', 'cyl']
        for i, var in enumerate(vars):
            diff = result.coef[i] - R_REFERENCE['coefficients'][i]
            print(f"{var:<12} {result.coef[i]:>14.5f} {R_REFERENCE['coefficients'][i]:>14.5f} {diff:>11.2e}")
        
        # Must match to within numerical precision of QR decomposition
        # R reference was printed to 7 decimals, so should be exact
        assert np.allclose(result.coef, R_REFERENCE['coefficients'], 
                          rtol=1e-5, atol=1e-5), \
            "Coefficients don't match R - REGULATORY COMPLIANCE FAILURE"
        
        # 1b. Fitted values (debugging - let's see what we're computing)
        print("\n" + "="*70)
        print("FITTED VALUES (first 10 observations)")
        print("="*70)
        print(f"{'Obs':<6} {'PyRegression':<15} {'R lm()':<15} {'Diff':<12}")
        print("-"*70)
        for i in range(10):
            diff = result.fitted_values[i] - R_REFERENCE['fitted_first10'][i]
            print(f"{i+1:<6} {result.fitted_values[i]:>14.5f} {R_REFERENCE['fitted_first10'][i]:>14.5f} {diff:>11.2e}")
        
        assert np.allclose(result.fitted_values[:10], R_REFERENCE['fitted_first10'],
                          rtol=1e-4, atol=1e-4), \
            "Fitted values don't match R"
        
        # 2. Residuals
        print("\n" + "="*70)
        print("RESIDUALS (first 10 observations)")
        print("="*70)
        print(f"{'Obs':<6} {'PyRegression':<15} {'R lm()':<15} {'Diff':<12}")
        print("-"*70)
        for i in range(10):
            diff = result.residuals[i] - R_REFERENCE['residuals_first10'][i]
            print(f"{i+1:<6} {result.residuals[i]:>14.5f} {R_REFERENCE['residuals_first10'][i]:>14.5f} {diff:>11.2e}")
        
        # If fitted values match, residuals must also match (y - fitted)
        assert np.allclose(result.residuals[:10], R_REFERENCE['residuals_first10'],
                          rtol=1e-4, atol=1e-4)
        
        # 3. Model fit statistics
        print("\n" + "="*70)
        print("MODEL FIT STATISTICS")
        print("="*70)
        
        # Compute R-squared
        ss_res = np.sum(result.residuals**2)
        ss_tot = np.sum((y - np.mean(y))**2)
        r_squared = 1 - (ss_res / ss_tot)
        
        # Compute residual standard error
        n = len(y)
        p = result.rank  # includes intercept
        residual_se = np.sqrt(ss_res / result.df_residual)
        
        print(f"R-squared:           {r_squared:.4f} (R: {R_REFERENCE['r_squared']:.4f})")
        print(f"Residual Std Error:  {residual_se:.3f} (R: {R_REFERENCE['residual_se']:.3f})")
        print(f"Degrees of Freedom:  {result.df_residual} (R: {R_REFERENCE['df_residual']})")
        print(f"Rank:                {result.rank} (R: {R_REFERENCE['rank']})")
        
        assert np.isclose(r_squared, R_REFERENCE['r_squared'], rtol=1e-3)
        assert np.isclose(residual_se, R_REFERENCE['residual_se'], rtol=1e-3)
        assert result.df_residual == R_REFERENCE['df_residual']
        assert result.rank == R_REFERENCE['rank']
        
        print("\n" + "="*70)
        print("✓ ALL CHECKS PASSED - REGULATORY GRADE EQUIVALENCE TO R")
        print("="*70 + "\n")
    
    @pytest.mark.parametrize('backend_name', ['cpu'])
    def test_all_backends_consistency(self, backend_name):
        """
        Test all available backends produce consistent results.
        
        GPU backends may use FP32, so we allow slightly looser tolerance,
        but they should still be statistically equivalent.
        """
        backends = list_available_backends()
        if backend_name not in backends:
            pytest.skip(f"Backend {backend_name} not available")
        
        # Prepare data
        y = MTCARS['mpg'].values
        X = MTCARS[['wt', 'hp', 'cyl']].values
        
        # Get CPU reference
        cpu_backend = get_backend('cpu')
        cpu_result = cpu_backend.fit_linear_model(X, y)
        
        # Test target backend
        backend = get_backend(backend_name)
        result = backend.fit_linear_model(X, y)
        
        print(f"\n{'='*70}")
        print(f"Testing {backend.name} backend")
        print(f"{'='*70}")
        
        # Coefficients should be very close
        max_coef_diff = np.max(np.abs(result.coef - cpu_result.coef))
        print(f"Max coefficient difference: {max_coef_diff:.2e}")
        
        # GPU FP32 allows ~1e-3, CPU/GPU FP64 should be ~1e-6
        if 'fp32' in backend.name or 'ridge' in backend.name:
            tolerance = 1e-3
            print(f"Using FP32/Ridge tolerance: {tolerance}")
        else:
            tolerance = 1e-6
            print(f"Using FP64 tolerance: {tolerance}")
        
        assert np.allclose(result.coef, cpu_result.coef, rtol=tolerance, atol=tolerance)
        assert np.allclose(result.residuals, cpu_result.residuals, rtol=tolerance, atol=tolerance)
        
        print(f"✓ {backend.name} matches CPU reference")
    
    def test_statistical_interpretation(self):
        """
        Test that we can perform real statistical analysis.
        
        This validates the complete workflow a statistician would use.
        """
        # Fit model
        y = MTCARS['mpg'].values
        X = MTCARS[['wt', 'hp', 'cyl']].values
        
        backend = get_backend('cpu')
        result = backend.fit_linear_model(X, y)
        
        # Compute standard errors (would normally be in a higher-level API)
        n = len(y)
        p = result.rank
        
        # Residual variance
        s2 = np.sum(result.residuals**2) / result.df_residual
        
        # (X'X)^(-1) via QR
        # We have QR decomposition: Q, R, pivot
        # (X'X)^(-1) = R^(-1) R^(-T) [with pivot reordering]
        R = result.qr_R[:p, :p]
        
        # Standard errors of coefficients
        # SE(β) = sqrt(diag((X'X)^(-1) * s^2))
        R_inv = np.linalg.inv(R)
        XtX_inv = R_inv @ R_inv.T
        se_beta = np.sqrt(np.diag(XtX_inv) * s2)
        
        # Reorder based on pivot
        pivot = result.qr_pivot[:p] - 1  # Convert to 0-indexed
        se_beta_ordered = np.zeros(p)
        se_beta_ordered[pivot] = se_beta
        
        # t-statistics
        t_stats = result.coef / se_beta_ordered
        
        print("\n" + "="*80)
        print("STATISTICAL SUMMARY (like R's summary.lm)")
        print("="*80)
        print(f"{'Coefficient':<15} {'Estimate':>12} {'Std.Error':>12} {'t value':>10}")
        print("-"*80)
        
        vars = ['Intercept', 'wt', 'hp', 'cyl']
        for i in range(p):
            print(f"{vars[i]:<15} {result.coef[i]:>12.5f} {se_beta_ordered[i]:>12.5f} {t_stats[i]:>10.3f}")
        
        # Compare to R's standard errors
        print("\n" + "="*80)
        print("STANDARD ERROR VALIDATION")
        print("="*80)
        print(f"{'Variable':<12} {'PyRegression':<15} {'R lm()':<15} {'Diff':<12}")
        print("-"*80)
        for i, var in enumerate(vars):
            diff = se_beta_ordered[i] - R_REFERENCE['std_errors'][i]
            print(f"{var:<12} {se_beta_ordered[i]:>14.5f} {R_REFERENCE['std_errors'][i]:>14.5f} {diff:>11.2e}")
        
        # Standard errors must match R
        assert np.allclose(se_beta_ordered, R_REFERENCE['std_errors'], 
                          rtol=1e-3, atol=1e-3)
        
        print("\n" + "="*80)
        print("INTERPRETATION:")
        print("="*80)
        print("Vehicle weight (wt):  1000lb increase → -3.17 mpg (p < 0.001)")
        print("Horsepower (hp):      10hp increase → -0.18 mpg (p = 0.14, n.s.)")
        print("Cylinders (cyl):      1 cylinder increase → -0.94 mpg (p = 0.10)")
        print("\nThis matches the R output exactly - we can publish this!")
        print("="*80 + "\n")
    
    def test_prediction_workflow(self):
        """
        Test prediction on new data (like a real analyst would do).
        """
        # Fit model on mtcars
        y = MTCARS['mpg'].values
        X = MTCARS[['wt', 'hp', 'cyl']].values
        
        backend = get_backend('cpu')
        result = backend.fit_linear_model(X, y)
        
        # Predict for a new car: wt=3.0, hp=120, cyl=6
        new_car = np.array([[3.0, 120, 6]])
        
        # Add intercept
        X_new = np.column_stack([np.ones(1), new_car])
        
        # Predict
        prediction = X_new @ result.coef
        
        print("\n" + "="*70)
        print("PREDICTION EXAMPLE")
        print("="*70)
        print(f"New car specifications:")
        print(f"  Weight:      3.0 (1000 lbs)")
        print(f"  Horsepower:  120 hp")
        print(f"  Cylinders:   6")
        print(f"\nPredicted MPG: {prediction[0]:.2f}")
        
        # Manually verify
        manual_pred = (38.75179 + 
                      (-3.16697 * 3.0) + 
                      (-0.01804 * 120) + 
                      (-0.94162 * 6))
        
        print(f"Manual calculation: {manual_pred:.2f}")
        print(f"Difference: {abs(prediction[0] - manual_pred):.2e}")
        
        # Allow small numerical error from rounding in R reference values
        assert np.isclose(prediction[0], manual_pred, rtol=1e-3, atol=1e-3)
        
        print("\n✓ Prediction workflow validated")
        print("="*70 + "\n")


class TestGPUAcceleration:
    """Test GPU backends provide acceleration while maintaining accuracy."""
    
    def test_large_scale_regression(self):
        """
        Test GPU backends on larger problem where speedup is visible.
        
        This isn't about correctness (already tested), but about demonstrating
        the value proposition: "Same answers, 10x faster on GPU"
        """
        import time
        from pyregression._backends.precision_detector import detect_gpu_capabilities
        
        # Generate larger dataset (still realistic for clinical trials)
        np.random.seed(42)
        n = 10000  # 10K observations
        p = 20     # 20 predictors
        
        X = np.random.randn(n, p)
        beta_true = np.random.randn(p) * 2
        y = X @ beta_true + np.random.randn(n) * 0.5
        
        backends_to_test = ['cpu']
        
        # Test GPU backends based on actual hardware (not just what's in list)
        caps = detect_gpu_capabilities()
        if caps.gpu_type == 'nvidia':
            backends_to_test.append('pytorch')
        elif caps.gpu_type == 'mps':
            backends_to_test.append('mps')
        
        results = {}
        timings = {}
        
        print("\n" + "="*70)
        print("LARGE-SCALE REGRESSION BENCHMARK")
        print("="*70)
        print(f"Problem size: n={n:,}, p={p}")
        print()
        
        for backend_name in backends_to_test:
            backend = get_backend(backend_name)
            
            # Warm-up
            _ = backend.fit_linear_model(X, y)
            
            # Timed run
            start = time.time()
            result = backend.fit_linear_model(X, y)
            elapsed = time.time() - start
            
            results[backend_name] = result
            timings[backend_name] = elapsed
            
            print(f"{backend.name:<30} {elapsed*1000:>10.2f} ms")
        
        print()
        
        # Validate consistency
        cpu_result = results['cpu']
        for backend_name, result in results.items():
            if backend_name == 'cpu':
                continue
            
            # GPU should match CPU (within tolerance)
            if 'ridge' in backend_name or 'fp32' in get_backend(backend_name).name:
                tol = 1e-2
            else:
                tol = 1e-5
            
            max_diff = np.max(np.abs(result.coef - cpu_result.coef))
            
            if max_diff < tol:
                speedup = timings['cpu'] / timings[backend_name]
                print(f"✓ {backend_name}: {speedup:.1f}x speedup (max diff: {max_diff:.2e})")
            else:
                print(f"✗ {backend_name}: ACCURACY FAILURE (max diff: {max_diff:.2e})")
        
        print("="*70 + "\n")


if __name__ == '__main__':
    # Run with verbose output
    pytest.main([__file__, '-v', '-s'])